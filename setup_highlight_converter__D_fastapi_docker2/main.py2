from fastapi import FastAPI, File, UploadFile, HTTPException
from fastapi.responses import Response, HTMLResponse, StreamingResponse
from fastapi.staticfiles import StaticFiles
from fastapi.middleware.cors import CORSMiddleware
import pandas as pd
from datetime import datetime
import json
import os
import io
from typing import List
from pathlib import Path

app = FastAPI()




# Configure CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Create static directory if it doesn't exist
os.makedirs("static", exist_ok=True)

# Mount static files
app.mount("/static", StaticFiles(directory="static"), name="static")

def convert_json_to_csv(json_data: dict, filename: str) -> io.StringIO:
    """Convert JSON annotations to CSV format and return as StringIO."""
    try:
        csv_data = []
        annotations = json_data.get('annotations', [])

        for annotation in annotations:
            # Skip non-highlight annotations if any
            if annotation.get('type') != 'highlight':
                continue

            # Format the timestamp
            timestamp = annotation.get("timestamp", "")
            try:
                formatted_date = datetime.strptime(timestamp, "%Y-%m-%dT%H:%M:%S.%fZ").strftime("%Y-%m-%d %H:%M:%S") if timestamp else ""
            except ValueError:
                formatted_date = timestamp  # Keep original if parsing fails

            # Get chapter/section from toc_family_titles if available
            chapter = annotation.get("toc_family_titles", [""])[0]

            # Create entry
             #  "Location": annotation.get("spine_index", ""),
            csv_data.append({
                "Highlight": annotation.get("highlighted_text", ""),
                "Title": filename,
                "Chapter": chapter,
                "Note": annotation.get("notes", ""),
                "Location": "",
                "Date": formatted_date,
                "Style": annotation.get("style", {}).get("which", "")
            })

        # Create DataFrame
        df = pd.DataFrame(csv_data) if csv_data else pd.DataFrame(columns=['Highlight', 'Title', 'Chapter', 'Note', 'Location', 'Date', 'Style'])
        df = df[['Highlight', 'Title', 'Chapter', 'Note', 'Location', 'Date', 'Style']]

        # Convert to CSV in memory
        output = io.StringIO()
        df.to_csv(output, index=False, encoding='utf-8')
        output.seek(0)
        return output

    except Exception as e:
        raise Exception(f"Error converting JSON to CSV: {str(e)}")

@app.get("/", response_class=HTMLResponse)
async def read_root():
    with open("static/index.html", "r", encoding="utf-8") as f:
        return f.read()

@app.post("/upload/")
async def upload_files(files: List[UploadFile] = File(...)):
    try:
        # Process each file
        if len(files) == 1:
            # Single file case - return individual CSV
            file = files[0]
            if not file.filename.endswith('.json'):
                raise HTTPException(status_code=400, detail=f"File {file.filename} is not a JSON file")

            try:
                # Read JSON content
                content = await file.read()
                json_data = json.loads(content)

                # Get filename without extension and sanitize it
                filename = Path(file.filename).stem
                sanitized_filename = "".join(x for x in filename if x.isalnum() or x in (' ', '-', '_'))

                # Convert to CSV
                csv_io = convert_json_to_csv(json_data, filename)

                # Return CSV file
                return StreamingResponse(
                    iter([csv_io.getvalue()]),
                    media_type="text/csv",
                    headers={
                        "Content-Disposition": f'attachment; filename="{sanitized_filename}-highlights.csv"'
                    }
                )

            except json.JSONDecodeError:
                raise HTTPException(status_code=400, detail=f"File {file.filename} contains invalid JSON")
            except Exception as e:
                raise HTTPException(status_code=500, detail=f"Error processing file {file.filename}: {str(e)}")
        else:
            # Multiple files case - combine into single CSV
            all_data = []
            for file in files:
                if not file.filename.endswith('.json'):
                    raise HTTPException(status_code=400, detail=f"File {file.filename} is not a JSON file")

                try:
                    content = await file.read()
                    json_data = json.loads(content)
                    filename = Path(file.filename).stem
                    csv_io = convert_json_to_csv(json_data, filename)

                    # Skip header for all but first file
                    df = pd.read_csv(io.StringIO(csv_io.getvalue()), encoding='utf-8')
                    all_data.append(df)

                except json.JSONDecodeError:
                    raise HTTPException(status_code=400, detail=f"File {file.filename} contains invalid JSON")
                except Exception as e:
                    raise HTTPException(status_code=500, detail=f"Error processing file {file.filename}: {str(e)}")

            # Combine all DataFrames
            combined_df = pd.concat(all_data, ignore_index=True)

            # Convert to CSV
            output = io.StringIO()
            combined_df.to_csv(output, index=False, encoding='utf-8')
            output.seek(0)

            # Return combined CSV
            current_time = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
            return StreamingResponse(
                iter([output.getvalue()]),
                media_type="text/csv",
                headers={
                   "Content-Disposition": f'attachment; filename="combined-highlights_{current_time}.csv"'
                }
            )

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=187, reload=True)
